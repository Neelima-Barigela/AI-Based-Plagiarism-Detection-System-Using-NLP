{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLomMF5FRBvHhPEUg+8K76",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neelima-Barigela/AI-Based-Plagiarism-Detection-System-Using-NLP/blob/main/AI_Based_Plagiarism_Detection_System_Using_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI-Based Plagiarism Detection System Using NLP"
      ],
      "metadata": {
        "id": "tt-G3tFdgDbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on detecting plagiarism in textual documents using Natural Language Processing (NLP) techniques. The system takes multiple text files as input, cleans and processes the text, and converts it into numerical form using TF-IDF vectorization. It then measures the similarity between documents using Cosine Similarity. Based on a predefined threshold, the system determines whether plagiarism exists between any pair of documents. This approach helps in identifying copied or highly similar content in an efficient and automated way."
      ],
      "metadata": {
        "id": "n9kpQWmGgJPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import Required Libraries**\n",
        "\n",
        "Import all necessary Python libraries for text processing, vectorization, and similarity computation."
      ],
      "metadata": {
        "id": "78yO_CNIg7Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVCcd5r9g5p6",
        "outputId": "3116c4b2-1d53-477d-fa77-49e41072192b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# Step 2: Load Text Documents**\n",
        "\n",
        "### Read multiple text files from the dataset to be used for plagiarism comparison."
      ],
      "metadata": {
        "id": "vbm3YW0ShBRh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5gDomcD1cjyP"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/Plagiarism-checker-Python-master.zip\"\n",
        "extract_path = \"/content/plagiarism_data\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder = \"/content/plagiarism_data/Plagiarism-checker-Python-master\"\n",
        "\n",
        "documents = []\n",
        "file_names = []\n",
        "\n",
        "for file in os.listdir(folder):\n",
        "    if file.endswith(\".txt\"):\n",
        "        with open(os.path.join(folder, file), 'r', encoding='utf-8') as f:\n",
        "            documents.append(f.read())\n",
        "            file_names.append(file)"
      ],
      "metadata": {
        "id": "zQLyIYBUcxpH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Text Preprocessing\n",
        "\n",
        "Clean the text by converting to lowercase and removing unwanted characters."
      ],
      "metadata": {
        "id": "14Ymo3JThrhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z ]', '', text)\n",
        "    words = text.split()\n",
        "    words = [w for w in words if w not in stopwords.words('english')]\n",
        "    return \" \".join(words)\n",
        "\n",
        "cleaned_docs = [clean_text(doc) for doc in documents]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA3eAPLCdD25",
        "outputId": "ebea5cad-f00d-442c-fc20-e446a807cde4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Files:\", file_names)\n",
        "print(\"Number of documents:\", len(documents))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mggUEaRsejR_",
        "outputId": "785d4a77-e728-43b6-c644-8e338206e1f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files: ['fatma.txt', 'juma.txt', 'requirements.txt', 'john.txt']\n",
            "Number of documents: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "file_names = []\n",
        "\n",
        "for file in os.listdir(folder):\n",
        "    if file.endswith(\".txt\") and file != \"requirements.txt\":\n",
        "        with open(os.path.join(folder, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            documents.append(f.read())\n",
        "            file_names.append(file)\n",
        "\n",
        "print(file_names)\n",
        "print(len(documents))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCqNYIMVe2ZH",
        "outputId": "5d9d0280-eafb-4c44-e82b-8253c574cb34"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fatma.txt', 'juma.txt', 'john.txt']\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_docs = [clean_text(doc) for doc in documents]\n",
        "\n",
        "for name, doc in zip(file_names, cleaned_docs):\n",
        "    print(name, \"->\", doc[:150])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwifLCIIe7I2",
        "outputId": "91524ddd-f2c7-4f46-8af0-1b0d96fb366e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatma.txt -> life best trying tofind works taking time intrying pursue skills\n",
            "juma.txt -> life finding money use things makes happycoz life kinda short\n",
            "john.txt -> life finding money spending luxury stuffscoz life kinda short trust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Text Vectorization (TF-IDF)\n",
        "\n",
        "Convert cleaned text documents into numerical vectors using TF-IDF technique."
      ],
      "metadata": {
        "id": "tXKYShcPh7Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(cleaned_docs)\n",
        "\n",
        "print(\"TF-IDF shape:\", tfidf_matrix.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt7lKTn1fAfv",
        "outputId": "2e89177e-5262-4a08-aa99-001166be76fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF shape: (3, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Cosine Similarity Calculation\n",
        "\n",
        "Measure the similarity between each pair of documents using cosine similarity."
      ],
      "metadata": {
        "id": "SHYqxkbtiG4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "print(similarity_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-tx-HxifOOF",
        "outputId": "ecdab80b-291e-4d92-979b-493f52d30767"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Matrix:\n",
            "[[1.         0.0821799  0.0821799 ]\n",
            " [0.0821799  1.         0.48111972]\n",
            " [0.0821799  0.48111972 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Plagiarism Detection\n",
        "\n",
        "Compare similarity scores with a threshold to identify plagiarism."
      ],
      "metadata": {
        "id": "-7Y2C5QaiVTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.8\n",
        "\n",
        "print(\"Plagiarism Detection Results:\\n\")\n",
        "\n",
        "for i in range(len(file_names)):\n",
        "    for j in range(i+1, len(file_names)):\n",
        "        score = similarity_matrix[i][j]\n",
        "\n",
        "        if score >= threshold:\n",
        "            print(f\"‚ö†Ô∏è Plagiarism detected between {file_names[i]} and {file_names[j]} \"\n",
        "                  f\"(Similarity = {score:.2f})\")\n",
        "        else:\n",
        "            print(f\"‚úÖ No plagiarism between {file_names[i]} and {file_names[j]} \"\n",
        "                  f\"(Similarity = {score:.2f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpRVxpqUfOKu",
        "outputId": "e21dfbe3-3825-47e6-d1df-ff577b73e183"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plagiarism Detection Results:\n",
            "\n",
            "‚úÖ No plagiarism between fatma.txt and juma.txt (Similarity = 0.08)\n",
            "‚úÖ No plagiarism between fatma.txt and john.txt (Similarity = 0.08)\n",
            "‚úÖ No plagiarism between juma.txt and john.txt (Similarity = 0.48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üèÅ Conclusion**\n",
        "\n",
        "## The plagiarism detection system successfully analyzed all the given documents and calculated their similarity scores. Since all similarity values were below the selected plagiarism threshold, the system correctly identified that no plagiarism exists between any of the document pairs. This demonstrates that the implemented NLP-based approach using TF-IDF and cosine similarity is effective for detecting similarity and identifying original content."
      ],
      "metadata": {
        "id": "mXVqB3yygb6V"
      }
    }
  ]
}